# Serverless AI Content Pipeline ğŸ¤–ğŸ“°

A fully serverless AI-powered content pipeline that automatically fetches news articles, processes them into vector embeddings, and generates personalized newsletters using AWS Bedrock and Pinecone.

## ğŸ¯ Overview

This project implements a complete RAG (Retrieval-Augmented Generation) pipeline with three main workflows:

1. **News Fetching** - Scheduled collection of news articles from NewsAPI
2. **Embeddings Processing** - Automatic vectorization and storage in Pinecone
3. **Newsletter Generation** - AI-powered newsletter creation and email delivery

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Serverless AI Pipeline                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Fetch News   â”‚    â”‚  Process     â”‚    â”‚  Generate    â”‚  â”‚
â”‚  â”‚   Lambda     â”‚â”€â”€â”€â–¶â”‚  Embeddings  â”‚    â”‚ Newsletter   â”‚  â”‚
â”‚  â”‚ (Scheduled)  â”‚    â”‚   Lambda     â”‚    â”‚   Lambda     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â”‚                    â”‚                    â”‚          â”‚
â”‚         â–¼                    â–¼                    â–¼          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   S3 Bucket  â”‚    â”‚   Pinecone   â”‚    â”‚   AWS SES    â”‚  â”‚
â”‚  â”‚  (Raw Data)  â”‚    â”‚  (Vectors)   â”‚    â”‚   (Email)    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Features

- **Automated News Collection**: Scheduled fetching from NewsAPI every hour
- **Vector Search**: Semantic search powered by Pinecone vector database
- **AI Content Generation**: Newsletter creation using AWS Bedrock (Llama 3.3 70B)
- **Email Delivery**: Automated sending via AWS SES
- **Clean Architecture**: Use case pattern with dependency injection
- **Infrastructure as Code**: AWS CDK for complete infrastructure management
- **Serverless**: Zero server management, pay-per-use pricing

## ğŸ“‹ Prerequisites

- **Node.js** 22.x or later
- **AWS Account** with appropriate permissions
- **AWS CLI** configured with credentials
- **Pinecone Account** and API key
- **NewsAPI Key**

### AWS Services Required

- Lambda (Node.js 22 runtime)
- S3 (for raw data storage)
- EventBridge (for scheduling)
- SES (for email delivery)
- Bedrock (for AI models)
- SSM Parameter Store (for secrets)

## ğŸ› ï¸ Setup

### 1. Clone the Repository

```bash
git clone https://github.com/gutkedu/serverless-ai-content-pipe.git
cd serverless-ai-content-pipe
```

### 2. Install Dependencies

```bash
# Install backend dependencies
cd backend/nodejs
npm install

# Install infrastructure dependencies
cd ../../infra
npm install
```

### 3. Configure AWS Parameters

Store your secrets in AWS Systems Manager Parameter Store:

```bash
# Pinecone API Key
aws ssm put-parameter \
  --name "/ai-content-pipe/pinecone-api-key" \
  --value "your-pinecone-api-key" \
  --type "SecureString" \
  --profile your-aws-profile

# NewsAPI Key
aws ssm put-parameter \
  --name "/ai-content-pipe/news-api-key" \
  --value "your-news-api-key" \
  --type "SecureString" \
  --profile your-aws-profile

# From Email (must be verified in SES)
aws ssm put-parameter \
  --name "/ai-content-pipe/from-email" \
  --value "your-verified-email@example.com" \
  --type "String" \
  --profile your-aws-profile
```

### 4. Verify Email in SES

If you're in SES sandbox mode, verify your sender email:

```bash
aws ses verify-email-identity \
  --email-address your-email@example.com \
  --region us-east-1 \
  --profile your-aws-profile
```

### 5. Create Pinecone Index

Create an index named `ai-content-pipe` with:
- **Dimensions**: 1536 (for Amazon Titan Embeddings v1)
- **Metric**: Cosine similarity

Or use the infrastructure Lambda to create it automatically.

## ğŸš¢ Deployment

### Deploy Infrastructure

```bash
cd infra

# Build the project
npm run build

# Deploy stateful resources (S3, Lambda layers)
cdk deploy AiContentPipeStatefulStack --profile your-aws-profile

# Deploy stateless resources (Lambdas, EventBridge)
cdk deploy AiContentPipeStatelessStack --profile your-aws-profile
```

### Deploy All at Once

```bash
cdk deploy --all --profile your-aws-profile
```

## ğŸ“– Usage

### Fetch News (Automated)

Runs automatically every hour via EventBridge. To trigger manually:

```bash
aws lambda invoke \
  --function-name FetchNewsScheduledLambda \
  --region us-east-1 \
  --profile your-aws-profile \
  response.json
```

### Process Embeddings (Automated)

Triggered automatically when new files are added to S3. To trigger manually:

```bash
aws lambda invoke \
  --function-name ProcessNewsEmbeddingsLambda \
  --payload '{"Records":[{"s3":{"bucket":{"name":"your-bucket"},"object":{"key":"news-data/file.json"}}}]}' \
  --region us-east-1 \
  --profile your-aws-profile \
  response.json
```

### Generate Newsletter

Use the Function URL or invoke directly:

```bash
curl -X POST "https://your-function-url.lambda-url.us-east-1.on.aws/" \
  -H "Content-Type: application/json" \
  -d '{
    "topic": "artificial intelligence",
    "recipients": ["recipient@example.com"],
    "maxArticles": 5
  }'
```

Or via AWS CLI:

```bash
aws lambda invoke \
  --function-name GenerateNewsletterLambda \
  --payload '{
    "topic": "technology trends",
    "recipients": ["your-email@example.com"],
    "maxArticles": 10
  }' \
  --region us-east-1 \
  --profile your-aws-profile \
  response.json
```

## ğŸ—ï¸ Project Structure

```
serverless-ai-content-pipe/
â”œâ”€â”€ backend/nodejs/          # Lambda functions and business logic
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ lambdas/        # Lambda handlers
â”‚   â”‚   â”‚   â”œâ”€â”€ fetch-news-scheduled.ts
â”‚   â”‚   â”‚   â”œâ”€â”€ process-news-embeddings.ts
â”‚   â”‚   â”‚   â””â”€â”€ generate-newsletter.ts
â”‚   â”‚   â”œâ”€â”€ use-cases/      # Business logic layer
â”‚   â”‚   â”‚   â”œâ”€â”€ fetch-news.ts
â”‚   â”‚   â”‚   â”œâ”€â”€ process-news-embeddings.ts
â”‚   â”‚   â”‚   â”œâ”€â”€ generate-newsletter.ts
â”‚   â”‚   â”‚   â””â”€â”€ factories/  # Dependency injection
â”‚   â”‚   â”œâ”€â”€ providers/      # External service abstractions
â”‚   â”‚   â”‚   â”œâ”€â”€ ai/         # Bedrock provider
â”‚   â”‚   â”‚   â”œâ”€â”€ bucket/     # S3 provider
â”‚   â”‚   â”‚   â”œâ”€â”€ email/      # SES provider
â”‚   â”‚   â”‚   â””â”€â”€ news-api/   # NewsAPI provider
â”‚   â”‚   â”œâ”€â”€ repositories/   # Data access layer
â”‚   â”‚   â”‚   â””â”€â”€ pinecone/   # Pinecone vector repository
â”‚   â”‚   â””â”€â”€ shared/         # Utilities and helpers
â”‚   â””â”€â”€ layers/             # Lambda layers (dependencies)
â”œâ”€â”€ infra/                  # AWS CDK infrastructure
â”‚   â”œâ”€â”€ bin/                # CDK app entry point
â”‚   â”œâ”€â”€ lib/                # Stack definitions
â”‚   â”‚   â”œâ”€â”€ stateful-stack.ts
â”‚   â”‚   â””â”€â”€ stateless-stack.ts
â”‚   â””â”€â”€ cdk.json           # CDK configuration
â””â”€â”€ docs/                   # Documentation
```

## ğŸ”§ Configuration

### Environment Variables

Lambdas use the following environment variables:

- `MODEL_ID`: Bedrock model ID (default: `us.meta.llama3-3-70b-instruct-v1:0`)
- `PINECONE_INDEX`: Pinecone index name (default: `ai-content-pipe`)
- `NEWS_BUCKET_NAME`: S3 bucket for raw news data

### Models Used

- **Embeddings**: Amazon Titan Text Embeddings v1 (1536 dimensions)
- **Generation**: Meta Llama 3.3 70B Instruct (cross-region inference profile)

## ğŸ§ª Testing

### Run TypeScript Build

```bash
cd backend/nodejs
npm run build
```

### Check for Errors

```bash
npm run lint
```

### CloudWatch Metrics

Monitor in AWS Console:
- Lambda invocations and errors
- S3 object counts
- SES email delivery rates
- Bedrock API usage

## ğŸ’¡ Key Concepts

### Clean Architecture

The project follows a clean architecture pattern:

```
Handler â†’ Factory â†’ Use Case â†’ Providers/Repositories
```

- **Handlers**: Thin Lambda entry points
- **Factories**: Dependency injection for use cases
- **Use Cases**: Business logic orchestration
- **Providers**: External service abstractions (Bedrock, SES, S3, NewsAPI)
- **Repositories**: Data access (Pinecone)

### RAG Pipeline

1. **Ingestion**: News articles fetched and stored in S3
2. **Processing**: Articles converted to embeddings via Titan
3. **Storage**: Vectors stored in Pinecone with metadata
4. **Retrieval**: Semantic search finds relevant articles
5. **Generation**: Bedrock generates newsletter content
6. **Delivery**: SES sends formatted emails

## ğŸ¤ Contributing

Contributions are welcome! Please follow these steps:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ™ Acknowledgments

- AWS Bedrock for AI capabilities
- Pinecone for vector database
- NewsAPI for news content
- AWS CDK for infrastructure as code